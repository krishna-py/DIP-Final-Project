{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "import random\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the DRIVE dataset\n",
    "dataset = Path('data/DRIVE')\n",
    "\n",
    "# Load the training dataset\n",
    "train_images = sorted(dataset.glob('training/images/*.tif'))\n",
    "train_labels = sorted(dataset.glob('training/1st_manual/*.gif'))\n",
    "train_mask = sorted(dataset.glob('training/mask/*.gif'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_images), train_images[:3], train_labels[:3], train_mask[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the test dataset\n",
    "test_images = sorted(dataset.glob('test/images/*.tif'))\n",
    "test_mask = sorted(dataset.glob('test/mask/*.gif'))\n",
    "len(test_images), test_images[:3], test_mask[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a sample image, mask and label\n",
    "image = Image.open(train_images[0])\n",
    "mask = Image.open(train_mask[0])\n",
    "label = Image.open(train_labels[0])\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.subplot(131)\n",
    "plt.imshow(image)\n",
    "plt.title(f'Image {image.size}, {image.mode}')\n",
    "plt.subplot(132)\n",
    "plt.imshow(mask, cmap='gray')\n",
    "plt.title(f'Mask {mask.size}, {mask.mode}')\n",
    "plt.subplot(133)\n",
    "plt.imshow(label, cmap='gray')\n",
    "plt.title(f'Label {label.size}, {label.mode}')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying the image in R,G,B channels\n",
    "red, green, blue = image.split()\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.subplot(141)\n",
    "plt.imshow(image)\n",
    "plt.subplot(142)\n",
    "plt.imshow(red)\n",
    "plt.title('red')\n",
    "plt.subplot(143)\n",
    "plt.imshow(green)\n",
    "plt.title('green')\n",
    "plt.subplot(144)\n",
    "plt.imshow(blue)\n",
    "plt.title('blue')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing the preprocessing method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing training images\n",
    "\n",
    "# 1. take only the green channel\n",
    "# 2. apply morphological opening with a three-pixel diameter disk structuring element\n",
    "# 3. The local background gray level is computed by applying a 69Ã—69 mean filter to the image. The\n",
    "# background is then subtracted and the resulting gray levels are scaled from 0 to 1.\n",
    "# 4. a constant is added to the image gray levels so the mode gray level value in image is set to 0.5\n",
    "# 5.  top-hat transformation on the complement of the image using an eight-pixel radius\n",
    "# disk as the structuring element\n",
    "# \n",
    "\n",
    "import numpy as np\n",
    "from skimage import morphology\n",
    "from skimage import exposure\n",
    "from skimage import filters\n",
    "from skimage import img_as_float\n",
    "from skimage import transform\n",
    "\n",
    "def preprocess(image, mask):\n",
    "    # Convert the image and mask to float32 tensors\n",
    "    image = img_as_float(image)\n",
    "    mask = img_as_float(mask)\n",
    "    \n",
    "    # Take only the green channel\n",
    "    image = image[:, :, 1]\n",
    "    \n",
    "    # Apply morphological opening with a 3-pixel disk structuring element\n",
    "    selem = morphology.disk(3)\n",
    "    image = morphology.opening(image, selem)\n",
    "    \n",
    "    # Compute the local mean of the image\n",
    "    local_mean = filters.rank.mean(image, selem)\n",
    "    \n",
    "    # Subtract the local mean from the image\n",
    "    image = image - local_mean\n",
    "    \n",
    "    # Scale the image so that its values range from 0 to 1\n",
    "    image = exposure.rescale_intensity(image)\n",
    "    \n",
    "    # Add a constant to the image so that its minimum value is 0\n",
    "    image = image - image.min()\n",
    "    \n",
    "    # Normalize the image so its values sum to 1\n",
    "    image = image / image.sum()\n",
    "    \n",
    "    # Apply a top-hat transformation to the image\n",
    "    selem = morphology.disk(8)\n",
    "    image = morphology.white_tophat(image, selem)\n",
    "    \n",
    "    # Normalize the image so its values sum to 1\n",
    "    image = image / image.sum()\n",
    "    \n",
    "    # Apply the mask to the image\n",
    "    image = image * mask\n",
    "    \n",
    "    # # Convert the image and the mask to PyTorch tensors\n",
    "    # image = torch.from_numpy(image).unsqueeze(0)\n",
    "    # mask = torch.from_numpy(mask).unsqueeze(0)\n",
    "    \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_image = preprocess(image, mask)\n",
    "print(p_image.shape)\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.subplot(121)\n",
    "plt.title(f'Preprocessed image {p_image.shape}')\n",
    "plt.imshow(p_image, cmap='gray')\n",
    "plt.subplot(122)\n",
    "plt.title(f'ground truth {label.size}')\n",
    "plt.imshow(label, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature extraction from preprocessed image\n",
    "from numpy.lib.stride_tricks import sliding_window_view\n",
    "\n",
    "windows = sliding_window_view(p_image, (9, 9))\n",
    "windows.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_windows = sliding_window_view(label, (9, 9))\n",
    "label_windows.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizing the windows randomly\n",
    "\n",
    "plt.figure(figsize=(80, 16))\n",
    "for i in range(40):\n",
    "    xidx = random.randint(0, windows.shape[0])\n",
    "    yidx = random.randint(0, windows.shape[1])\n",
    "\n",
    "    if i < 20:\n",
    "        plt.subplot(4, 20, i+1)\n",
    "        plt.imshow(windows[xidx, yidx], cmap='gray')\n",
    "\n",
    "        plt.subplot(4, 20, i+21)\n",
    "        plt.imshow(label_windows[xidx, yidx], cmap='gray')\n",
    "\n",
    "        if i == 0:\n",
    "            plt.title(f'window {xidx}, {yidx}')\n",
    "            plt.title(f'label {xidx}, {yidx}')\n",
    "\n",
    "    if i >= 20:\n",
    "        plt.subplot(4, 20, i-20+41)\n",
    "        plt.imshow(windows[xidx, yidx], cmap='gray')\n",
    "\n",
    "        plt.subplot(4, 20, i-20+61)\n",
    "        plt.imshow(label_windows[xidx, yidx], cmap='gray')\n",
    "\n",
    "        if i == 20:\n",
    "            plt.title(f'window {xidx}, {yidx}')\n",
    "            plt.title(f'label {xidx}, {yidx}')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating the features of the image\n",
    "# for each image in the dataset, implement the below steps\n",
    "# 1. take the image and mask and preprocess the image\n",
    "# 2. Compute the following features on the preprocessed image, in a 9x9 window around each pixel in the image.\n",
    "#   a. intensity of center pixel\n",
    "#  b. absolute difference between the intensity of center pixel and min, max and mean intensity of the window\n",
    "# c. standard deviation of the window\n",
    "\n",
    "def extract_features(p_image):\n",
    "    # Compute the window\n",
    "    windows = sliding_window_view(p_image, (9, 9)).copy()\n",
    "\n",
    "    # pixel centers\n",
    "    centers = windows[:, :, 4, 4]\n",
    "\n",
    "    # min, max, mean, and standard deviation of each window\n",
    "    w_min, w_max, w_mean, w_std = windows.min(axis=(2, 3)), windows.max(axis=(2, 3)), windows.mean(axis=(2, 3)), windows.std(axis=(2, 3))\n",
    "\n",
    "    # features\n",
    "    features = np.stack([centers, np.abs(centers - w_min), np.abs(centers - w_max), np.abs(centers - w_mean), w_std], axis=2)\n",
    "    \n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_labels(label):\n",
    "    # Compute the window\n",
    "    windows = sliding_window_view(label, (9, 9)).copy()\n",
    "\n",
    "    # pixel centers\n",
    "    centers = windows[:, :, 4, 4]\n",
    "\n",
    "    # labels\n",
    "    labels = (centers > 0).astype(np.uint8)\n",
    "    \n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_values = extract_labels(label)\n",
    "label.size, label_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image_path, mask_path, label_path in zip(train_images, train_mask, train_labels):\n",
    "    print(image_path, mask_path, label_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = []\n",
    "\n",
    "for image_path, mask_path in zip(train_images, train_mask):\n",
    "    image = Image.open(image_path)\n",
    "    mask = Image.open(mask_path)\n",
    "\n",
    "    p_image = preprocess(image, mask)\n",
    "    print(p_image.shape)\n",
    "    features = extract_features(p_image)\n",
    "    print(features.shape)\n",
    "    train_features.append(features)\n",
    "    print(f'Extracted {features.shape} features from {image_path.name}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = []\n",
    "for label_path in train_labels:\n",
    "    labels = extract_labels(label)\n",
    "    print(labels.shape)\n",
    "    train_y.append(labels)\n",
    "    print(f'Extracted {labels.shape} labels from {label_path.name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = np.stack(train_features), np.stack(train_y)\n",
    "train_set[0].shape, train_set[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for all images in test set, extract the features\n",
    "test_features = []\n",
    "for image_path, mask_path in zip(test_images, test_mask):\n",
    "    image = Image.open(image_path)\n",
    "    mask = Image.open(mask_path)\n",
    "\n",
    "    p_image = preprocess(image, mask)\n",
    "    features = extract_features(p_image)\n",
    "    test_features.append(features)\n",
    "    print(f'Extracted {features.shape} features from {image_path.name}')\n",
    "\n",
    "test_set = np.stack(test_features)\n",
    "test_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save and load the train_set and test_set\n",
    "\n",
    "np.savez_compressed('data/DRIVE/train_set.npz', X=train_set[0], y=train_set[1])\n",
    "np.savez_compressed('data/DRIVE/test_set.npz', X=test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling postive and negative examples from train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the train_set and test_set\n",
    "train_set = np.load('data/DRIVE/train_set.npz', allow_pickle=True)\n",
    "test_set = np.load('data/DRIVE/test_set.npz', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set['X'].shape, train_set['y'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(train_set['y'], return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(train_set['y'].ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positive samples\n",
    "pos_samples = train_set['X'][train_set['y'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variance of negative samples\n",
    "\n",
    "neg_samples = train_set['X'][train_set['y'] == 0]\n",
    "neg_samples.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the std of negative samples\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.hist(neg_samples[:, -1].ravel(), bins=100)\n",
    "plt.title('std of negative samples')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampling the negative samples equal to the number of positive samples\n",
    "sample_neg_idx = np.random.choice(np.arange(neg_samples.shape[0]), size=train_set['y'].sum())\n",
    "sample_neg = neg_samples[sample_neg_idx]\n",
    "sample_neg.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the std of negative samples\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.hist(sample_neg[:, -1].ravel(), bins=100)\n",
    "plt.title('std of negative samples')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# balanced dataset = positive samples + negative samples\n",
    "balanced_x = np.concatenate([pos_samples, sample_neg], axis=0)\n",
    "balanced_y = np.concatenate([np.ones(pos_samples.shape[0]), np.zeros(sample_neg.shape[0])], axis=0)\n",
    "balanced_x.shape, balanced_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature scaling of balanced_x\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "balanced_x = scaler.fit_transform(balanced_x.reshape(-1, 5)).reshape(balanced_x.shape)\n",
    "balanced_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_y.sum(), np.unique(balanced_y, return_counts=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez_compressed('data/DRIVE/balanced_train_set.npz', X=balanced_x, y=balanced_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Neural Network using MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchmetrics import Accuracy\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch dataloader for balanced data\n",
    "\n",
    "class DRIVE(Dataset):\n",
    "    def __init__(self, M=30000):\n",
    "        super().__init__()\n",
    "        balanced_set = np.load('data/DRIVE/balanced_train_set.npz')\n",
    "        samples = np.random.choice(np.arange(balanced_set['X'].shape[0]), size=M)\n",
    "        self.X = balanced_set['X'][samples].astype(np.float32)\n",
    "        self.y = balanced_set['y'].reshape(-1,1)[samples].astype(np.float32)\n",
    "\n",
    "        print('Loaded the dataset', self.X.shape, self.X.dtype, self.y.shape, self.y.dtype)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP with 5 inputs, three hidden layers with 15 nodes each, and one output\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size=5, hidden_size=[15,15,15], output_size=1, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.p_dropout = dropout\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size[0])\n",
    "        for i in range(1, len(hidden_size)):\n",
    "            setattr(self, f'fc{i+1}', nn.Linear(hidden_size[i-1], hidden_size[i]))\n",
    "        self.fc4 = nn.Linear(hidden_size[-1], output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = F.relu6(self.fc1(x))\n",
    "        x = F.relu6(self.fc2(x))\n",
    "        x = F.relu6(self.fc3(x))\n",
    "        x = torch.sigmoid(self.fc4(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "epochs = 10\n",
    "BATCH_SIZE = 1024\n",
    "lr = 0.01\n",
    "\n",
    "train_set = DRIVE()\n",
    "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    losses = []\n",
    "    for batch_num, input_data in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        x, y = input_data\n",
    "        x = x.to(device).float()\n",
    "        y = y.to(device)\n",
    "\n",
    "        output = model(x)\n",
    "        loss = criterion(output, y)\n",
    "        loss.backward()\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_num % 40 == 0:\n",
    "            print('\\tEpoch %d | Batch %d | Loss %6.2f' % (epoch, batch_num, loss.item()))\n",
    "    print('Epoch %d | Loss %6.2f' % (epoch, sum(losses)/len(losses)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Pytorch Lightning for training the MLP classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning as L\n",
    "from torchmetrics import Accuracy\n",
    "\n",
    "class MLPLightning(L.LightningModule):\n",
    "    def __init__(self, input_size=5, hidden_size=[15,15,15], output_size=1, learning_rate=0.01):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.lr = learning_rate\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size[0])\n",
    "        for i in range(1, len(hidden_size)):\n",
    "            setattr(self, f'fc{i+1}', nn.Linear(hidden_size[i-1], hidden_size[i]))\n",
    "        self.fc4 = nn.Linear(hidden_size[-1], output_size)\n",
    "        self.acc = Accuracy(task='binary')\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = F.relu6(self.fc1(x))\n",
    "        x = F.relu6(self.fc2(x))\n",
    "        x = F.relu6(self.fc3(x))\n",
    "        x = torch.sigmoid(self.fc4(x))\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        output = self(x)\n",
    "        loss = F.binary_cross_entropy(output, y)\n",
    "        self.log('train_loss', loss)\n",
    "\n",
    "        acc = self.acc(output, y)\n",
    "        self.log('train_acc', acc)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        output = self(x)\n",
    "        loss = F.binary_cross_entropy(output, y)\n",
    "        self.log('val_loss', loss)\n",
    "\n",
    "        acc = self.acc(output, y)\n",
    "        self.log('val_acc', acc)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.SGD(self.parameters(), lr=self.lr, momentum=0.9)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, min_lr=1e-4)\n",
    "        return {\n",
    "            'optimizer': optimizer,\n",
    "            'lr_scheduler': {\n",
    "                'scheduler': scheduler,\n",
    "                'monitor': 'val_loss'\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name | Type           | Params\n",
      "----------------------------------------\n",
      "0 | fc1  | Linear         | 90    \n",
      "1 | fc2  | Linear         | 240   \n",
      "2 | fc3  | Linear         | 240   \n",
      "3 | fc4  | Linear         | 16    \n",
      "4 | acc  | BinaryAccuracy | 0     \n",
      "----------------------------------------\n",
      "586       Trainable params\n",
      "0         Non-trainable params\n",
      "586       Total params\n",
      "0.002     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded the dataset (30000, 5) float32 (30000, 1) float32\n",
      "training set: 24000 validation set: 6000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bf9880bed9d429ba5591491aed616e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/krishna.py/miniconda/envs/torch-env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
      "/Users/krishna.py/miniconda/envs/torch-env/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5725a7703114e1d9e490e154ca8674a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af68020fa32e4411893788b95e4bea80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7926560078542e494e83fab8d225ca9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "add97732d26f4c889c5624ee9120b987",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8ef08b7271f43e09dc8e36bc42ede5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "143578e4656d4b429f70f1d45658f25d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0caae00b5e5543e2a7866e56fc48f262",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23b938b5a86449f398cd6c4b1bfa6b39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "031b346148e7404090714a5daf60a7ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ea1549a5fee48cca4f7b79497a7d87a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cc924c46054459b954290311e0806f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "`model` must be a `LightningModule` or `torch._dynamo.OptimizedModule`, got `DataLoader`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/krishna.py/code/iiit-h_pdm_dip/final-project/segmentation.ipynb Cell 47\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/krishna.py/code/iiit-h_pdm_dip/final-project/segmentation.ipynb#Y114sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m trainer\u001b[39m.\u001b[39mfit(model, train_loader, val_loader)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/krishna.py/code/iiit-h_pdm_dip/final-project/segmentation.ipynb#Y114sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m \u001b[39m# final eval score\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/krishna.py/code/iiit-h_pdm_dip/final-project/segmentation.ipynb#Y114sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mvalidate(val_loader)\n",
      "File \u001b[0;32m~/miniconda/envs/torch-env/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py:640\u001b[0m, in \u001b[0;36mTrainer.validate\u001b[0;34m(self, model, dataloaders, ckpt_path, verbose, datamodule)\u001b[0m\n\u001b[1;32m    636\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m    637\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m`Trainer.validate()` requires a `LightningModule` when it hasn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt been passed in a previous run\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    638\u001b[0m         )\n\u001b[1;32m    639\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 640\u001b[0m     model \u001b[39m=\u001b[39m _maybe_unwrap_optimized(model)\n\u001b[1;32m    641\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39m_lightning_module \u001b[39m=\u001b[39m model\n\u001b[1;32m    642\u001b[0m _verify_strategy_supports_compile(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy)\n",
      "File \u001b[0;32m~/miniconda/envs/torch-env/lib/python3.10/site-packages/lightning/pytorch/utilities/compile.py:131\u001b[0m, in \u001b[0;36m_maybe_unwrap_optimized\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[39mreturn\u001b[39;00m model\n\u001b[1;32m    130\u001b[0m _check_mixed_imports(model)\n\u001b[0;32m--> 131\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m    132\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m`model` must be a `LightningModule` or `torch._dynamo.OptimizedModule`, got `\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(model)\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m`\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    133\u001b[0m )\n",
      "\u001b[0;31mTypeError\u001b[0m: `model` must be a `LightningModule` or `torch._dynamo.OptimizedModule`, got `DataLoader`"
     ]
    }
   ],
   "source": [
    "# Use the PyTorch Lightning Trainer to train the model\n",
    "\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 1024 * 1024\n",
    "SPLIT = 0.8\n",
    "lr = 0.001\n",
    "\n",
    "# use 20% of training data for validation\n",
    "trainval_set = DRIVE()\n",
    "train_set_size = int(len(trainval_set) * SPLIT)\n",
    "val_set_size = len(trainval_set) - train_set_size\n",
    "print('training set:', train_set_size, 'validation set:', val_set_size)\n",
    "\n",
    "# split the train set into two\n",
    "seed = torch.Generator().manual_seed(42)\n",
    "train_set, val_set = random_split(trainval_set, [train_set_size, val_set_size], generator=seed)\n",
    "\n",
    "# data loaders\n",
    "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# create the model and train\n",
    "model = MLPLightning()\n",
    "\n",
    "callbacks = [\n",
    "    L.pytorch.callbacks.LearningRateMonitor(logging_interval='step'),\n",
    "    L.pytorch.callbacks.ModelCheckpoint(monitor='val_loss'),\n",
    "    L.pytorch.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
    "]\n",
    "trainer = L.Trainer(default_root_dir=\"models/mlpL\", max_epochs=EPOCHS, val_check_interval=0.1)\n",
    "trainer.fit(model, train_loader, val_loader)\n",
    "\n",
    "# final eval score\n",
    "trainer.validate(model, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save(model.state_dict(), 'data/DRIVE/model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Prediction of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
